{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df6898fc-1cc3-4789-90f3-59c09e52bba3",
   "metadata": {},
   "source": [
    "## 1. Import requests and BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb198e-9d11-468a-8fa8-0e20259943c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install BeautifulSoup by typing 'pip3 install beautifulsoup4' in command prompt or terminal then pressing enter\n",
    "#Install requests by typing 'pip3 install requests' then pressing enter\n",
    "#Beautiful Soup allows us to use the html and grab different data and we can use the data for whatever our goal is\n",
    "#requests module allows us to download the html \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c6395-05d8-457f-80c2-d63a971fc636",
   "metadata": {},
   "source": [
    "## 2. Fetching the HTML content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc374aa9-06ea-48f8-a21f-101af13dc140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a response variable and use a get request to get the page\n",
    "# A status code of 200 (<Response [200]>) means that the request was successful.\n",
    "res = requests.get('https://news.ycombinator.com/')    \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eff001-77d9-4635-b11d-3dacd8382439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the entire html file from the site we are scraping\n",
    "#res.text: This returns the content of the response as a string. \n",
    "#It's suitable for text-based content like HTML, XML, JSON, etc. \n",
    "\n",
    "\n",
    "print(res.text)     # res.text contains the HTML content of the web page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86e1f31-e72b-4149-ab67-37e058cd56c3",
   "metadata": {},
   "source": [
    "## 3. Parsing with Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd3bffb-89bc-4742-805b-edfc592de4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have the HTML content as a string, \n",
    "#Let's pass it to Beautiful Soup. Beautiful Soup then parses the HTML and creates a parse tree\n",
    "#This represents the structure of the HTML document.\n",
    "\n",
    "soup = BeautifulSoup(res.text, 'html.parser') # Create a BeautifulSoup object named 'soup' by parsing the HTML content\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77852e01-d774-436b-bc4e-edbb00e8b8c8",
   "metadata": {},
   "source": [
    "## 4. Navigating and extracting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6a944-dfe0-4214-86f1-cf6a8e13ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.body)  # Print the <body> tag and its contents of the parsed HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473a332e-5d77-4c9f-b83f-ad1ae7610fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the contents of the <body> tag\n",
    "# The .contents attribute returns a list of the children of the <body> tag\n",
    "\n",
    "print(soup.body.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad1de5-8ab3-40f0-b469-2d799c950742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find all the div objects\n",
    "#a <div> is a fundamental HTML element used to create divisions or sections within a web page\n",
    "\n",
    "print(soup.find_all('div'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba4e282-7403-4a5a-8955-0921e02eff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get all the 'a' tags (all the links in the page)\n",
    "#<a> tags are used to create hyperlinks, also known as anchor links. The term \"a\" stands for \"anchor.\"\n",
    "\n",
    "print(soup.find_all('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f4c138-89a4-451e-a749-85b06cb041a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the title tag\n",
    "print(soup.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e874723-735d-4ef2-8979-17e681b358be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the first <a> tag that comes up\n",
    "print(soup.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6fa056-7e72-47ff-b703-406b7a3b0974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the first item\n",
    "\n",
    "print(soup.find('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f7e0b-46e0-41e7-80d9-ad29e4e7c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to the first link on the webpage being scraped and right click and inspect it\n",
    "# Use the id attribute and use it to find the score with the same tag\n",
    "\n",
    "print(soup.find(id=\"score_40287020\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b349eb7-8a7e-4ef2-ace7-8423a8e2ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab data using a CSS selector\n",
    "#Let's grab span tags with scores\n",
    "\n",
    "print(soup.select('.score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62c3e84-e6c4-4acf-bff0-21176135ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select('#score_40287020'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6013b0-8569-4a45-a9bd-405ae3e71690",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select and print all elements with the CSS class \"titleline\" from the parsed HTML document\n",
    "\n",
    "print(soup.select('.titleline'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc64488-3f7a-4710-84ad-3c349e333436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have a list, let's grab the first item\n",
    "# Print the first element with class 'titleline'\n",
    "# soup.select('.titleline') selects all elements with class 'titleline'\n",
    "# [0] selects the first element from the list of elements with class 'titleline'\n",
    "\n",
    "print(soup.select('.titleline')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a2290-7a0b-4d65-8a9b-a96a91fa2d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all <a> tags that are direct children of elements with class 'titleline'\n",
    "# The .select() method returns a list of elements that match the CSS selector '.titleline > a\n",
    "\n",
    "links = soup.select('.titleline > a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a592b468-cf7d-447e-831e-2d285f021acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select all elements with class 'score'\n",
    "# The .select() method returns a list of elements that match the CSS selector '.score'\n",
    "votes = soup.select('.score')\n",
    "\n",
    "# Print the first element from the list of elements with class 'score'\n",
    "# This will print the first element with class 'score' from the parsed HTML document\n",
    "print(votes[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38456579-108e-498f-ac70-bc908063cc6c",
   "metadata": {},
   "source": [
    "## Customized Hacker News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b089c2-2a76-4ba4-a9a3-e144916357cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_hn(links, votes):\n",
    "    # Initialize an empty list to store the custom Hacker News data\n",
    "    hn = []\n",
    "    \n",
    "    # Iterate through the links and votes using enumerate to get both index and item\n",
    "    for idx, item in enumerate(links):\n",
    "        # Get the title of the story from the link\n",
    "        title = links[idx].getText()\n",
    "        # Get the URL of the story from the link\n",
    "        href = links[idx].get('href', None)\n",
    "        # Extract the points (votes) from the corresponding element in the votes list\n",
    "        # Remove ' points' from the text and convert it to an integer\n",
    "        points = int(votes[idx].getText().replace(' points', ''))\n",
    "        \n",
    "        # Print the points for debugging purposes\n",
    "        print(points)\n",
    "        \n",
    "        # Append a dictionary containing title, link, and points to the hn list\n",
    "        hn.append({'title': title, 'link': href, 'points': points})\n",
    "    \n",
    "    # Return the list of custom Hacker News data\n",
    "    return hn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32723a1d-96ff-4218-9de6-dd268f653422",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_custom_hn(links,votes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b04b0ed-dc54-4563-8ce2-840b14fd766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make a few changes because the above code can give an error for stories with no scores/votes\n",
    "res = requests.get('https://news.ycombinator.com/')\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "links = soup.select('.titleline > a')\n",
    "subtext = soup.select('.subtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4525b621-57bd-430f-a1ed-b99b8e57d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_hn(links,subtext):\n",
    "    hn = []\n",
    "    for idx, item in enumerate(links):\n",
    "        title = links[idx].getText()\n",
    "        href = links[idx].get('href', None)\n",
    "        vote = subtext[idx].select('.score')\n",
    "        if len(vote):\n",
    "          points = int(votes[idx].getText().replace(' points', ''))\n",
    "          if points > 99:\n",
    "            hn.append({'title': title, 'link':href, 'votes': points})\n",
    "    return hn\n",
    "\n",
    "print(create_custom_hn(links,subtext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae3de70-069e-4bfa-ac4a-e67c24cb4ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0802736f-ecc4-4305-81a7-01165fa27dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(create_custom_hn(links,subtext))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ae85f2-a464-4178-a21b-daa5be4376a7",
   "metadata": {},
   "source": [
    "# Final app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7b6ea-ba6d-4703-95ae-14592be5cb86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "# Send a GET request to the Hacker News website\n",
    "res = requests.get('https://news.ycombinator.com/')\n",
    "\n",
    "# Parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "# Select all links with class 'titleline'\n",
    "links = soup.select('.titleline > a')\n",
    "\n",
    "# Select all elements with class 'subtext'\n",
    "subtext = soup.select('.subtext')\n",
    "\n",
    "# Define a function to sort stories by their votes\n",
    "def sort_stories_by_votes(hnlist):\n",
    "    return sorted(hnlist, key=lambda k: k['votes'], reverse=True)\n",
    "\n",
    "# Define a function to create a custom Hacker News list\n",
    "def create_custom_hn(links, subtext):\n",
    "    hn = []\n",
    "    # Iterate through the links and subtext\n",
    "    for idx, item in enumerate(links):\n",
    "        # Get the title of the story\n",
    "        title = links[idx].getText()\n",
    "        # Get the URL of the story\n",
    "        href = links[idx].get('href', None)\n",
    "        # Get the votes (points) for the story\n",
    "        votes = subtext[idx].select('.score')\n",
    "        # Check if votes exist for the story\n",
    "        if len(votes):\n",
    "            # Get the points as an integer\n",
    "            points = int(votes[0].getText().replace(' points', ''))\n",
    "            # Check if the story has more than 99 points\n",
    "            if points > 99:\n",
    "                # Append the title, link, and votes to the hn list\n",
    "                hn.append({'title': title, 'link': href, 'votes': points})\n",
    "    # Return the sorted list of stories by votes\n",
    "    return sort_stories_by_votes(hn)\n",
    "\n",
    "# Print the custom Hacker News list\n",
    "pprint(create_custom_hn(links, subtext))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd567dd2-a8f4-4c56-9b16-735dfe06307b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
